import os
from dotenv import load_dotenv
import openai
import yfinance as yf
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import chromedriver_autoinstaller
from urllib.parse import urlparse

chromedriver_autoinstaller.install()


load_dotenv()
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

prompt_template = """
    ë„ˆëŠ” ì£¼ì‹ ì‹œì„¸ ì˜ˆì¸¡ ì „ë¬¸ê°€ì•¼. ì•„ë˜ëŠ” ì‚¼ì„±ì „ìì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ë“¤ê³¼ ì£¼ê°€ ì •ë³´ì•¼.
    ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ë‚´ì¼ ì‚¼ì„±ì „ìì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•´ì¤˜.

    ìµœê·¼ ë‰´ìŠ¤:
    {news_summary}

    ìµœê·¼ ì¢…ê°€ (ë§ˆì§€ë§‰ 5ì¼):
    {price_data}

    ì˜ˆì¸¡ ì¡°ê±´:
    - ë‚´ì¼(ì˜ì—…ì¼ ê¸°ì¤€) ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•´ì¤˜.
    - ì˜¤ì§ ìˆ«ì í•˜ë‚˜ë§Œ ì˜ˆì¸¡í•´ì¤˜.
    - ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ë§ˆ.
"""

def get_price_data(ticker="005930.KS", days=5):
    df = yf.download(ticker, period="7d", interval="1d")
    close_prices = df["Close"].dropna().tail(days).tolist()
    return [round(p) for p in close_prices]  # ì •ìˆ˜ ë³€í™˜

def predict_price_gpt(news_summary, price_data):
    prompt = prompt_template.format(
        news_summary=news_summary,
        price_data=price_data
    )

def get_news_urls(keyword="ì‚¼ì„±ì „ì", max_count=8):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    try:
        driver = webdriver.Chrome(options=options)
        print("âœ… driver ìƒì„± ì„±ê³µ")
    except Exception as e:
        print("âŒ driver ìƒì„± ì‹¤íŒ¨:", e)

    driver.get(f"https://search.naver.com/search.naver?where=news&query={keyword}")
    #driver.get("https://www.naver.com")
    print("ğŸ” í˜„ì¬ URL:", driver.current_url)
    print("ğŸ” í˜ì´ì§€ ì œëª©:", driver.title)

    with open("naver_test.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)

    print("ğŸ“„ HTML ê¸¸ì´:", len(driver.page_source))

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'a[target="_blank"]'))
        )
    except Exception as e:
        print("âŒ ëŒ€ê¸° ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
    finally:
        with open("debug.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("âœ… debug.html ì €ì¥ ì™„ë£Œ")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    links = []
    seen_domains = set()

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if not any(domain in href for domain in [
            "www.newsis.com",           # ë‰´ì‹œìŠ¤
            "www.yna.co.kr",            # ì—°í•©ë‰´ìŠ¤
            "www.hankyung.com",         # í•œêµ­ê²½ì œ    
            "www.mk.co.kr",             # ë§¤ì¼ê²½ì œ
            "www.hani.co.kr",           # í•œê²¨ë ˆ
            "www.khan.co.kr",           # ê²½í–¥ì‹ ë¬¸
            "www.donga.com",            # ë™ì•„ì¼ë³´
            "www.ohmynews.com",         # ì˜¤ë§ˆì´ë‰´ìŠ¤
        ]):
            continue

        # ì–¸ë¡ ì‚¬ ë„ë©”ì¸ë§Œ ì¶”ì¶œ
        netloc = urlparse(href).netloc
        domain = ".".join(netloc.split(".")[-2:])  # ì˜ˆ: newsis.com

        if domain not in seen_domains:
            seen_domains.add(domain)
            links.append(href)

        if len(links) >= max_count:
            break

    return links

def extract_news_body(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    body = soup.select_one("#dic_area")
    return body.text.strip() if body else ""


def summarize_articles_with_gpt(article_texts):
    full_text = "\n\n".join(article_texts)[:6000]  # ê¸¸ì´ ì œí•œ
    prompt = """
        ë„ˆëŠ” ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì•¼. ì•„ë˜ëŠ” ì‚¼ì„±ì „ì ê´€ë ¨ ê¸°ì‚¬ë“¤ì´ì•¼.
        ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ìµœê·¼ ì´ìŠˆë¥¼ 4ì¤„ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.

        {full_text}
        """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )

    return response.choices[0].message.content.strip()

def get_samsung_news_summary():
    urls = get_news_urls()
    articles = [extract_news_body(url) for url in urls]
    #summary = summarize_articles_with_gpt(articles)
    #return summary

print(get_news_urls())
#print(get_samsung_news_summary())