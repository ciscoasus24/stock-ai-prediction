import os
from dotenv import load_dotenv
import openai
import yfinance as yf
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import chromedriver_autoinstaller
from urllib.parse import urlparse

chromedriver_autoinstaller.install()


load_dotenv()
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

prompt_template = """
    ë„ˆëŠ” ì£¼ì‹ ì‹œì„¸ ì˜ˆì¸¡ ì „ë¬¸ê°€ì•¼. ì•„ë˜ëŠ” ì‚¼ì„±ì „ìì— ëŒ€í•œ ìµœê·¼ ë‰´ìŠ¤ë“¤ê³¼ ì£¼ê°€ ì •ë³´ì•¼.
    ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ë‚´ì¼ ì‚¼ì„±ì „ìì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•´ì¤˜.

    ìµœê·¼ ë‰´ìŠ¤:
    {news_summary}

    ìµœê·¼ ì¢…ê°€ (ë§ˆì§€ë§‰ 5ì¼):
    {price_data}

    ì˜ˆì¸¡ ì¡°ê±´:
    - ë‚´ì¼(ì˜ì—…ì¼ ê¸°ì¤€) ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•´ì¤˜.
    - ì˜¤ì§ ìˆ«ì í•˜ë‚˜ë§Œ ì˜ˆì¸¡í•´ì¤˜.
    - ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ë§ˆ.
"""

def get_price_data(ticker="005930.KS", days=5):
    df = yf.download(ticker, period="7d", interval="1d")
    close_prices = df["Close"].dropna().tail(days).tolist()
    return [round(p) for p in close_prices]  # ì •ìˆ˜ ë³€í™˜

def predict_price_gpt(news_summary, price_data):
    prompt = prompt_template.format(
        news_summary=news_summary,
        price_data=price_data
    )

def get_news_urls(keyword, max_count=8):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    try:
        driver = webdriver.Chrome(options=options)
        print("âœ… driver ìƒì„± ì„±ê³µ")
    except Exception as e:
        print("âŒ driver ìƒì„± ì‹¤íŒ¨:", e)

    driver.get(f"https://search.naver.com/search.naver?where=news&query={keyword}")
    print("ğŸ” í˜„ì¬ URL:", driver.current_url)
    print("ğŸ” í˜ì´ì§€ ì œëª©:", driver.title)

    with open("naver_test.html", "w", encoding="utf-8") as f:
        f.write(driver.page_source)

    print("ğŸ“„ HTML ê¸¸ì´:", len(driver.page_source))

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'a[target="_blank"]'))
        )
    except Exception as e:
        print("âŒ ëŒ€ê¸° ì¤‘ ì—ëŸ¬ ë°œìƒ:", e)
    finally:
        with open("debug.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("âœ… debug.html ì €ì¥ ì™„ë£Œ")

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    links = []
    seen_domains = set()

    for a in soup.find_all("a", href=True):
        href = a["href"]
        if not any(domain in href for domain in news_articles_domain):
            continue

        # ì–¸ë¡ ì‚¬ ë„ë©”ì¸ë§Œ ì¶”ì¶œ
        netloc = urlparse(href).netloc
        domain = ".".join(netloc.split(".")[-2:])  # ì˜ˆ: newsis.com

        if domain not in seen_domains:
            seen_domains.add(domain)
            links.append(href)

        if len(links) >= max_count:
            break

    return links

def extract_news_body(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    body = soup.select_one("#dic_area")
    return body.text.strip() if body else ""


def summarize_articles_with_gpt(article_texts):
    full_text = "\n\n".join(article_texts)[:6000]  # ê¸¸ì´ ì œí•œ
    prompt = """
        ë„ˆëŠ” ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì•¼. ì•„ë˜ëŠ” ì‚¼ì„±ì „ì ê´€ë ¨ ê¸°ì‚¬ë“¤ì´ì•¼.
        ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ìµœê·¼ ì´ìŠˆë¥¼ 4ì¤„ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜.

        {full_text}
        """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )

    return response.choices[0].message.content.strip()

def get_samsung_news_summary():
    urls = get_news_urls()
    articles = [extract_news_body(url) for url in urls]
    #summary = summarize_articles_with_gpt(articles)
    #return summary

samsung_related_keywords = [
    "ì‚¼ì„±ì „ì ë°˜ë„ì²´",
    "ì‚¼ì„±ì „ì íŒŒìš´ë“œë¦¬",
    "ì‚¼ì„±ì „ì ê°ì‚°",
    "ì‚¼ì„±ì „ì ìˆ˜ìœ¨",
    "ì‚¼ì„±ì „ì ì‹¤ì  ì „ë§",
    "ì‚¼ì„±ì „ì ì• í”Œ",
    "ì‚¼ì„±ì „ì TSMC",
    "ì‚¼ì„±ì „ì ì™¸êµ­ì¸ ë§¤ìˆ˜",
    "ì‚¼ì„±ì „ì ëª©í‘œì£¼ê°€",
    "ì‚¼ì„±ì „ì ê³µê¸‰ë§",
    "ì´ì¬ìš©"
]

news_articles_domain = [
            "www.newsis.com",           # ë‰´ì‹œìŠ¤
            "www.yna.co.kr",            # ì—°í•©ë‰´ìŠ¤
            "www.hankyung.com",         # í•œêµ­ê²½ì œ    
            "www.mk.co.kr",             # ë§¤ì¼ê²½ì œ
            "www.hani.co.kr",           # í•œê²¨ë ˆ
            "www.khan.co.kr",           # ê²½í–¥ì‹ ë¬¸
            "www.donga.com",            # ë™ì•„ì¼ë³´
            "www.ohmynews.com",         # ì˜¤ë§ˆì´ë‰´ìŠ¤
        ]

def classifier(urls):
    handlers = {
        "newsis": get_article_text_from_newsis,
        "yna": get_article_text_from_yna,
        "hani": get_article_text_from_hani,
        "donga": get_article_text_from_donga,
        "mk": get_article_text_from_mk,
        "khan": get_article_text_from_khan,
        "ohmynews": get_article_text_from_ohmynews,
        "hankyung": get_article_text_from_hankyung,
    }

    for url in urls:
        for domain, handler in handlers.items():
            if domain in url:
                return handler(url)


def get_article_text_from_newsis(url):
    print("from newsis")
    # ScraperAPI ìš”ì²­
    payload = {
        'api_key': '1814c4b2082de44045d1d0af9243ab75',
        'url': url,
        'country_code': 'kr',
        'device_type': 'desktop'
    }
    r = requests.get('https://api.scraperapi.com/', params=payload)

    # ì‘ë‹µ HTML íŒŒì‹±
    soup = BeautifulSoup(r.text, 'html.parser')

    # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ
    container = soup.select_one("article")
    if not container:
        return "âŒ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

    # <br> íƒœê·¸ê°€ ë¬¸ë‹¨ ê²½ê³„ë‹ˆê¹Œ ì¤„ë°”ê¿ˆ ìœ ì§€í•´ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = container.get_text(separator="\n", strip=True)
    return text

def get_article_text_from_yna(url):
    print("from yna")
    # ScraperAPI ìš”ì²­
    payload = {
        'api_key': '1814c4b2082de44045d1d0af9243ab75',
        'url': url,
        'country_code': 'kr',
        'device_type': 'desktop'
    }
    r = requests.get('https://api.scraperapi.com/', params=payload)

    # ì‘ë‹µ HTML íŒŒì‹±
    soup = BeautifulSoup(r.text, 'html.parser')

    # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ
    container = soup.select_one(".story-news.article")
    if not container:
        return "âŒ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

    # <br> íƒœê·¸ê°€ ë¬¸ë‹¨ ê²½ê³„ë‹ˆê¹Œ ì¤„ë°”ê¿ˆ ìœ ì§€í•´ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = container.get_text(separator="\n", strip=True)
    return text
    return

def get_article_text_from_hani(url):
    print("from hani")
    return

def get_article_text_from_donga(url):
    print("from donga")
    return

def get_article_text_from_mk(url):
    print("from mk")
    return

def get_article_text_from_khan(url):
    print("from khan")
    return

def get_article_text_from_ohmynews(url):
    print("from ohmynews")
    return

def get_article_text_from_hankyung(url):
    print("from hankyung")
    # ScraperAPI ìš”ì²­
    payload = {
        'api_key': '1814c4b2082de44045d1d0af9243ab75',
        'url': url,
        'country_code': 'kr',
        'device_type': 'desktop'
    }
    r = requests.get('https://api.scraperapi.com/', params=payload)

    
    # ì‘ë‹µ HTML íŒŒì‹±
    soup = BeautifulSoup(r.text, 'html.parser')

    # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ
    container = soup.select_one("#articletxt")
    if not container:
        return "âŒ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

    # <br> íƒœê·¸ê°€ ë¬¸ë‹¨ ê²½ê³„ë‹ˆê¹Œ ì¤„ë°”ê¿ˆ ìœ ì§€í•´ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = container.get_text(separator="\n", strip=True)
    return text

urls = get_news_urls("ì‚¼ì„±ì „ì")
a = "https://www.yna.co.kr/view/AKR20250801024300003?input=1195m"
#print(get_article_text_from_hankyung(a))
print(urls)
print(get_article_text_from_yna(a))
#print(classifier(urls))